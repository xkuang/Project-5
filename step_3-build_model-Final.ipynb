{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import chdir \n",
    "chdir('./lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from project_5 import general_process, load_data_from_database, make_data_dict, general_model, general_transformer, general_modelGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain and Data\n",
    "\n",
    "MADELON is an artificial dataset, which was part of the NIPS 2003 feature selection challenge. This is a two-class classification problem with continuous input variables. The difficulty is that the problem is multivariate and highly non-linear.\n",
    "MADELON is an artificial dataset containing data points grouped in 32 clusters placed on the vertices of a five dimensional hypercube and randomly labeled +1 or -1. The five dimensions constitute 5 informative features. 15 linear combinations of those features were added to form a set of 20 (redundant) informative features. Based on those 20 features one must separate the examples into the 2 classes (corresponding to the +-1 labels). A number of distractor feature called 'probes' were added having no predictive power. The order of the features and patterns were randomized.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "We are going to further filter our features using a simple Grid Search Cross Validation model.\n",
    "\n",
    "### Solution Statement\n",
    "\n",
    "In order to do this, we are going to run a Grid Search Cross Validation through our data\n",
    "We will construct a Pipeline that uses SelectKBest to transform data\n",
    "We will construct a Pipeline that uses LogisticRegression to model data\n",
    "We will construct a Pipeline that uses KNearestNeighbors to model data\n",
    "And Gridsearch optimal parameters for logistic regression and KNN\n",
    "\n",
    "### Metric\n",
    "\n",
    "Our metric is our accuracy score on our feature selection\n",
    "\n",
    "### Benchmark\n",
    "\n",
    "Our benchmark hasn't changed since step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Implement the following code pipeline using the functions you write in `lib/project_5.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/build_model.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 502)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use the function we wrote to load our data into the dataframe\n",
    "df = load_data_from_database()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Columns: 502 entries, index to label\n",
      "dtypes: int64(502)\n",
      "memory usage: 7.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Let's get info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>feat_000</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>999.500000</td>\n",
       "      <td>481.722500</td>\n",
       "      <td>483.452500</td>\n",
       "      <td>510.166000</td>\n",
       "      <td>483.384500</td>\n",
       "      <td>501.612500</td>\n",
       "      <td>479.259000</td>\n",
       "      <td>480.109500</td>\n",
       "      <td>476.565000</td>\n",
       "      <td>486.793500</td>\n",
       "      <td>...</td>\n",
       "      <td>478.811500</td>\n",
       "      <td>486.356500</td>\n",
       "      <td>496.565500</td>\n",
       "      <td>493.49950</td>\n",
       "      <td>510.893000</td>\n",
       "      <td>478.219500</td>\n",
       "      <td>483.309000</td>\n",
       "      <td>507.977000</td>\n",
       "      <td>490.266000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>577.494589</td>\n",
       "      <td>6.421769</td>\n",
       "      <td>30.186294</td>\n",
       "      <td>38.899165</td>\n",
       "      <td>9.059895</td>\n",
       "      <td>41.389418</td>\n",
       "      <td>6.795956</td>\n",
       "      <td>40.575925</td>\n",
       "      <td>1.384461</td>\n",
       "      <td>15.043836</td>\n",
       "      <td>...</td>\n",
       "      <td>4.011735</td>\n",
       "      <td>23.967366</td>\n",
       "      <td>127.635442</td>\n",
       "      <td>34.81902</td>\n",
       "      <td>37.459353</td>\n",
       "      <td>5.880613</td>\n",
       "      <td>13.559847</td>\n",
       "      <td>37.224297</td>\n",
       "      <td>25.825273</td>\n",
       "      <td>1.00025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>453.000000</td>\n",
       "      <td>371.000000</td>\n",
       "      <td>459.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>471.000000</td>\n",
       "      <td>430.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>463.000000</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>368.00000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>457.000000</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>-1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>499.750000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>452.750000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>471.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>470.00000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>474.000000</td>\n",
       "      <td>474.000000</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>-1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>999.500000</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>510.500000</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>487.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>492.00000</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1499.250000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>503.000000</td>\n",
       "      <td>536.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>484.000000</td>\n",
       "      <td>506.250000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>496.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>481.000000</td>\n",
       "      <td>502.000000</td>\n",
       "      <td>586.000000</td>\n",
       "      <td>517.00000</td>\n",
       "      <td>535.000000</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>533.000000</td>\n",
       "      <td>507.250000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>503.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>654.000000</td>\n",
       "      <td>519.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>505.000000</td>\n",
       "      <td>611.000000</td>\n",
       "      <td>481.000000</td>\n",
       "      <td>536.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>497.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>615.00000</td>\n",
       "      <td>661.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>535.000000</td>\n",
       "      <td>644.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index     feat_000     feat_001     feat_002     feat_003  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean    999.500000   481.722500   483.452500   510.166000   483.384500   \n",
       "std     577.494589     6.421769    30.186294    38.899165     9.059895   \n",
       "min       0.000000   462.000000   381.000000   370.000000   453.000000   \n",
       "25%     499.750000   477.000000   464.000000   485.000000   477.000000   \n",
       "50%     999.500000   482.000000   483.000000   510.500000   483.000000   \n",
       "75%    1499.250000   486.000000   503.000000   536.000000   490.000000   \n",
       "max    1999.000000   503.000000   600.000000   654.000000   519.000000   \n",
       "\n",
       "          feat_004     feat_005     feat_006     feat_007     feat_008  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean    501.612500   479.259000   480.109500   476.565000   486.793500   \n",
       "std      41.389418     6.795956    40.575925     1.384461    15.043836   \n",
       "min     371.000000   459.000000   334.000000   471.000000   430.000000   \n",
       "25%     475.000000   475.000000   452.750000   476.000000   477.000000   \n",
       "50%     500.000000   479.000000   480.000000   477.000000   487.000000   \n",
       "75%     528.000000   484.000000   506.250000   477.000000   496.250000   \n",
       "max     688.000000   505.000000   611.000000   481.000000   536.000000   \n",
       "\n",
       "          ...         feat_491     feat_492     feat_493    feat_494  \\\n",
       "count     ...      2000.000000  2000.000000  2000.000000  2000.00000   \n",
       "mean      ...       478.811500   486.356500   496.565500   493.49950   \n",
       "std       ...         4.011735    23.967366   127.635442    34.81902   \n",
       "min       ...       463.000000   391.000000   130.000000   368.00000   \n",
       "25%       ...       476.000000   471.000000   404.000000   470.00000   \n",
       "50%       ...       479.000000   486.000000   504.000000   492.00000   \n",
       "75%       ...       481.000000   502.000000   586.000000   517.00000   \n",
       "max       ...       497.000000   566.000000   920.000000   615.00000   \n",
       "\n",
       "          feat_495     feat_496     feat_497     feat_498     feat_499  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean    510.893000   478.219500   483.309000   507.977000   490.266000   \n",
       "std      37.459353     5.880613    13.559847    37.224297    25.825273   \n",
       "min     398.000000   457.000000   435.000000   363.000000   403.000000   \n",
       "25%     486.000000   474.000000   474.000000   482.000000   473.000000   \n",
       "50%     511.000000   478.000000   483.000000   508.000000   490.000000   \n",
       "75%     535.000000   482.000000   492.000000   533.000000   507.250000   \n",
       "max     661.000000   500.000000   535.000000   644.000000   583.000000   \n",
       "\n",
       "            label  \n",
       "count  2000.00000  \n",
       "mean      0.00000  \n",
       "std       1.00025  \n",
       "min      -1.00000  \n",
       "25%      -1.00000  \n",
       "50%       0.00000  \n",
       "75%       1.00000  \n",
       "max       1.00000  \n",
       "\n",
       "[8 rows x 502 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the Data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import samples_generator\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[-2.0060363 ,  0.96262702, -2.25549538, ...,  1.52381869,\n",
       "         -1.48860853, -0.81983392],\n",
       "        [-1.37226453, -1.87040739, -0.33279748, ...,  1.38890779,\n",
       "          1.34491786, -0.07652201],\n",
       "        [ 0.43515865, -0.52667565,  0.63968679, ...,  1.16405627,\n",
       "         -0.02484371,  1.23796644],\n",
       "        ..., \n",
       "        [-0.31597826,  0.59310079, -0.3699152 , ...,  1.88358111,\n",
       "         -1.71690213,  2.50550886],\n",
       "        [ 1.84354035,  1.10819796, -0.23629141, ..., -0.2749934 ,\n",
       "         -0.60229222, -1.15628037],\n",
       "        [-0.69154671,  1.48892195, -1.6170706 , ..., -1.93889458,\n",
       "          0.49888865, -1.79005158]]),\n",
       " 'X_train': array([[ 1.06893041, -0.19074272,  0.78815767, ...,  1.63624445,\n",
       "         -0.96487617,  0.82327663],\n",
       "        [-0.104721  ,  0.61549632, -0.54065671, ...,  1.79364051,\n",
       "         -1.44832143,  0.91716866],\n",
       "        [-0.92627699, -1.20973928,  0.06065035, ...,  0.98417507,\n",
       "          0.49888865,  0.07214038],\n",
       "        ..., \n",
       "        [-0.19861312,  0.07800363, -0.28083267, ...,  1.72618505,\n",
       "         -1.15288266,  1.15189874],\n",
       "        [-1.30184545,  0.54830973, -1.12711669, ...,  0.12973932,\n",
       "         -0.36056959,  0.18168108],\n",
       "        [ 1.25671464,  1.86964594, -0.04327926, ..., -0.83712218,\n",
       "         -1.00516327,  0.24427577]]),\n",
       " 'processes': [StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  SelectKBest(k=20, score_func=<function f_classif at 0x1148e66e0>),\n",
       "  GridSearchCV(cv=None, error_score='raise',\n",
       "         estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "         fit_params={}, iid=True, n_jobs=1,\n",
       "         param_grid={'C': [0.01, 0.02, 0.03, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1]},\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "         scoring=None, verbose=0)],\n",
       " 'test_score': 0.61199999999999999,\n",
       " 'train_score': 0.6226666666666667,\n",
       " 'transform': SelectKBest(k=20, score_func=<function f_classif at 0x1148e66e0>),\n",
       " 'y_test': 532     1\n",
       " 80     -1\n",
       " 936     1\n",
       " 1411    1\n",
       " 814     1\n",
       " 1614   -1\n",
       " 1498   -1\n",
       " 927    -1\n",
       " 597     1\n",
       " 1501    1\n",
       " 869    -1\n",
       " 1484    1\n",
       " 1247    1\n",
       " 635     1\n",
       " 974     1\n",
       " 1530   -1\n",
       " 1695    1\n",
       " 1246   -1\n",
       " 925     1\n",
       " 1078   -1\n",
       " 3       1\n",
       " 106    -1\n",
       " 1531   -1\n",
       " 1729    1\n",
       " 616     1\n",
       " 1869   -1\n",
       " 1753    1\n",
       " 1762    1\n",
       " 1485   -1\n",
       " 780    -1\n",
       "        ..\n",
       " 835     1\n",
       " 1538   -1\n",
       " 535    -1\n",
       " 1705    1\n",
       " 134     1\n",
       " 197    -1\n",
       " 1273    1\n",
       " 874     1\n",
       " 1108   -1\n",
       " 933    -1\n",
       " 992    -1\n",
       " 215    -1\n",
       " 1205   -1\n",
       " 1031   -1\n",
       " 1536   -1\n",
       " 6       1\n",
       " 1106   -1\n",
       " 955    -1\n",
       " 202     1\n",
       " 256    -1\n",
       " 1587   -1\n",
       " 293     1\n",
       " 1625    1\n",
       " 1675    1\n",
       " 286     1\n",
       " 1162   -1\n",
       " 192     1\n",
       " 297    -1\n",
       " 231     1\n",
       " 466    -1\n",
       " Name: label, dtype: int64,\n",
       " 'y_train': 1422   -1\n",
       " 104     1\n",
       " 100    -1\n",
       " 732     1\n",
       " 612    -1\n",
       " 435    -1\n",
       " 727     1\n",
       " 478    -1\n",
       " 1218   -1\n",
       " 1789   -1\n",
       " 381     1\n",
       " 1893    1\n",
       " 70     -1\n",
       " 1832    1\n",
       " 534    -1\n",
       " 1784   -1\n",
       " 182     1\n",
       " 1297    1\n",
       " 1648    1\n",
       " 1276    1\n",
       " 1840   -1\n",
       " 524    -1\n",
       " 420    -1\n",
       " 700    -1\n",
       " 1081   -1\n",
       " 1065   -1\n",
       " 1534    1\n",
       " 1603   -1\n",
       " 157    -1\n",
       " 1672   -1\n",
       "        ..\n",
       " 1207    1\n",
       " 851     1\n",
       " 1665    1\n",
       " 584    -1\n",
       " 338     1\n",
       " 111    -1\n",
       " 553     1\n",
       " 1793    1\n",
       " 659    -1\n",
       " 853    -1\n",
       " 1296   -1\n",
       " 1113   -1\n",
       " 1123   -1\n",
       " 797    -1\n",
       " 1388   -1\n",
       " 920    -1\n",
       " 879     1\n",
       " 1134   -1\n",
       " 383    -1\n",
       " 1007    1\n",
       " 1691   -1\n",
       " 1739   -1\n",
       " 1763   -1\n",
       " 1419    1\n",
       " 701     1\n",
       " 1910   -1\n",
       " 263    -1\n",
       " 1263    1\n",
       " 377     1\n",
       " 1887   -1\n",
       " Name: label, dtype: int64}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load the data , run a Standard scaler and SelectKBest transformer through a \n",
    "# Grid Search with a Logistic regression...our own little pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "gs_params = {\n",
    "              'C': [.01, .02, .03, .05, .06, .07, .08, .09, .1]\n",
    "}\n",
    "\n",
    "this_dd = make_data_dict(df, random_state=0)\n",
    "this_scale = general_transformer(StandardScaler(), this_dd)\n",
    "this_kbest = general_transformer(SelectKBest(k=20), this_scale)\n",
    "gs = GridSearchCV(LogisticRegression(penalty='l1'), param_grid=gs_params)\n",
    "this_gs = general_modelGS(gs, this_dd)\n",
    "this_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6226666666666667, 0.61199999999999999)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_gs['train_score'], this_gs['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = this_kbest['transform'].get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 49,  56,  65, 106, 129, 162, 200, 205, 242, 283, 297, 337, 339,\n",
       "        348, 379, 443, 454, 473, 476, 494]),)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.where(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[-1.14661665, -0.71195696, -1.33243707, ...,  0.14735686,\n",
       "         -0.01614779, -0.38668972],\n",
       "        [-1.44914174, -0.9046903 ,  1.8322541 , ..., -1.10592215,\n",
       "         -0.80052878,  0.64174038],\n",
       "        [-0.40193949, -0.83241529, -1.21900728, ...,  0.55164041,\n",
       "         -0.43971353,  0.29893035],\n",
       "        ..., \n",
       "        [-1.12334548, -4.06069874, -0.47037066, ...,  1.26587469,\n",
       "         -1.67119169, -0.55809473],\n",
       "        [-0.84409155,  0.32398474,  0.50512555, ...,  1.05025679,\n",
       "         -1.50647168, -0.04387968],\n",
       "        [ 0.48236463,  0.97445977,  0.42572469, ..., -0.3377834 ,\n",
       "          0.2113227 , -0.55809473]]),\n",
       " 'X_train': array([[ 0.41255114,  0.27580141,  0.50512555, ..., -0.35125952,\n",
       "         -0.07105446,  0.29893035],\n",
       "        [ 0.11002605,  0.61308475,  1.37853494, ..., -1.01158932,\n",
       "         -0.03183541, -1.2437148 ],\n",
       "        [-0.56483762,  1.62493479, -0.15276724, ..., -0.7016386 ,\n",
       "          0.64273224, -1.58652484],\n",
       "        ..., \n",
       "        [-0.96044736,  1.16719311, -1.84287113, ...,  1.52192094,\n",
       "         -1.43587739,  0.12752533],\n",
       "        [ 0.99433017, -0.15784861, -0.89006088, ...,  1.10416127,\n",
       "          0.59566938, -1.07230979],\n",
       "        [-1.23970129,  0.54080975, -0.35694087, ...,  0.01259568,\n",
       "          1.395738  ,  1.49876547]]),\n",
       " 'processes': [StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  SelectKBest(k=20, score_func=<function f_classif at 0x1148e66e0>),\n",
       "  GridSearchCV(cv=None, error_score='raise',\n",
       "         estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "             weights='uniform'),\n",
       "         fit_params={}, iid=True, n_jobs=1,\n",
       "         param_grid={'n_neighbors': [5, 7, 9, 11, 13, 15, 17, 19, 21]},\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "         scoring=None, verbose=0)],\n",
       " 'test_score': 0.78400000000000003,\n",
       " 'train_score': 0.84466666666666668,\n",
       " 'transform': SelectKBest(k=20, score_func=<function f_classif at 0x1148e66e0>),\n",
       " 'y_test': 939     1\n",
       " 1926   -1\n",
       " 1767   -1\n",
       " 450     1\n",
       " 1099    1\n",
       " 1264   -1\n",
       " 1625    1\n",
       " 1968   -1\n",
       " 671    -1\n",
       " 480    -1\n",
       " 829    -1\n",
       " 1826   -1\n",
       " 1460    1\n",
       " 312     1\n",
       " 763    -1\n",
       " 1631   -1\n",
       " 1447    1\n",
       " 1443    1\n",
       " 1305   -1\n",
       " 1363    1\n",
       " 1747   -1\n",
       " 1498   -1\n",
       " 1438    1\n",
       " 1616    1\n",
       " 1246   -1\n",
       " 1729    1\n",
       " 930    -1\n",
       " 1116   -1\n",
       " 865     1\n",
       " 1399    1\n",
       "        ..\n",
       " 287     1\n",
       " 49     -1\n",
       " 1809   -1\n",
       " 1156    1\n",
       " 1727   -1\n",
       " 1982   -1\n",
       " 1709   -1\n",
       " 686     1\n",
       " 1230    1\n",
       " 1496    1\n",
       " 415     1\n",
       " 407     1\n",
       " 1377   -1\n",
       " 394     1\n",
       " 613     1\n",
       " 518    -1\n",
       " 196    -1\n",
       " 1872    1\n",
       " 501     1\n",
       " 615    -1\n",
       " 1222   -1\n",
       " 1997   -1\n",
       " 596     1\n",
       " 1037    1\n",
       " 1538   -1\n",
       " 58      1\n",
       " 1516   -1\n",
       " 1959    1\n",
       " 512     1\n",
       " 1121   -1\n",
       " Name: label, dtype: int64,\n",
       " 'y_train': 1384   -1\n",
       " 10     -1\n",
       " 1568    1\n",
       " 1385    1\n",
       " 978    -1\n",
       " 1514    1\n",
       " 1310   -1\n",
       " 1928   -1\n",
       " 567     1\n",
       " 909     1\n",
       " 1244   -1\n",
       " 584    -1\n",
       " 1052    1\n",
       " 1526   -1\n",
       " 1024    1\n",
       " 1201    1\n",
       " 1371   -1\n",
       " 1370   -1\n",
       " 1198    1\n",
       " 1236    1\n",
       " 1744    1\n",
       " 403    -1\n",
       " 744     1\n",
       " 1578    1\n",
       " 1401    1\n",
       " 1999    1\n",
       " 1891   -1\n",
       " 1112   -1\n",
       " 1145    1\n",
       " 904     1\n",
       "        ..\n",
       " 1703   -1\n",
       " 485     1\n",
       " 1853    1\n",
       " 826    -1\n",
       " 983     1\n",
       " 1064   -1\n",
       " 1737   -1\n",
       " 1308    1\n",
       " 1784   -1\n",
       " 836     1\n",
       " 223    -1\n",
       " 95      1\n",
       " 230     1\n",
       " 273     1\n",
       " 453    -1\n",
       " 418    -1\n",
       " 1915   -1\n",
       " 1695    1\n",
       " 1300   -1\n",
       " 1562   -1\n",
       " 190    -1\n",
       " 711     1\n",
       " 375    -1\n",
       " 261    -1\n",
       " 333     1\n",
       " 298     1\n",
       " 86      1\n",
       " 1408    1\n",
       " 1316    1\n",
       " 1360   -1\n",
       " Name: label, dtype: int64}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load the data , run a Standard scaler and SelectKBest transformer through a \n",
    "# Grid Search with a Logistic regression...our own little pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "gs_params = {\n",
    "              'n_neighbors': [5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "}\n",
    "\n",
    "this_dd = make_data_dict(df, random_state=0)\n",
    "this_scale = general_transformer(StandardScaler(), this_dd)\n",
    "this_kbest = general_transformer(SelectKBest(k=20), this_scale)\n",
    "gs = GridSearchCV(KNeighborsClassifier(), param_grid=gs_params)\n",
    "this_gs = general_modelGS(gs, this_dd)\n",
    "this_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.84466666666666668, 0.78400000000000003)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_gs['train_score'], this_gs['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 49,  57,  65, 106, 120, 129, 200, 206, 242, 286, 337, 339, 379,\n",
       "        385, 443, 454, 473, 476, 494, 497]),)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here are our most important features\n",
    "results = this_kbest['transform'].get_support()\n",
    "knnresults = np.where(results)\n",
    "knnresults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Conclusion:\n",
    "\n",
    "We constructed a Pipeline that uses StandardScaler and SelectKBest to transform the data, a Grid Search with an embedded LogisticRegression to model. Finally, we used Gridsearch optimal parameters for logistic regression and KNN classifier.\n",
    "Our accuracy score was much better. 84.4% Train and 78.4% Test. \n",
    "\n",
    "We discovered what I believe to be the most important features:\n",
    " \n",
    "9,  57,  65, 106, 120, 129, 200, 206, 242, 286, 337, 339, 379,\n",
    "85, 443, 454, 473, 476, 494, 49\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
