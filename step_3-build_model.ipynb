{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import chdir \n",
    "chdir('./lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ginodefalco/dsi/dsi_repo/DSI_SM_3/projects/project-05/lib\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from project_5 import general_process, load_data_from_database, make_data_dict, general_model, general_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain and Data\n",
    "\n",
    "MADELON is an artificial dataset, which was part of the NIPS 2003 feature selection challenge. This is a two-class classification problem with continuous input variables. The difficulty is that the problem is multivariate and highly non-linear.\n",
    "MADELON is an artificial dataset containing data points grouped in 32 clusters placed on the vertices of a five dimensional hypercube and randomly labeled +1 or -1. The five dimensions constitute 5 informative features. 15 linear combinations of those features were added to form a set of 20 (redundant) informative features. Based on those 20 features one must separate the examples into the 2 classes (corresponding to the +-1 labels). A number of distractor feature called 'probes' were added having no predictive power. The order of the features and patterns were randomized.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "We are going to further filter our features using a simple Grid Search Cross Validation model.\n",
    "\n",
    "### Solution Statement\n",
    "\n",
    "In order to do this, we are going to run a Grid Search Cross Validation through our data\n",
    "We will construct a Pipeline that uses SelectKBest to transform data\n",
    "We will construct a Pipeline that uses LogisticRegression to model data\n",
    "We will construct a Pipeline that uses KNearestNeighbors to model data\n",
    "And Gridsearch optimal parameters for logistic regression and KNN\n",
    "\n",
    "### Metric\n",
    "\n",
    "Our metric is our accuracy score on our feature selection\n",
    "\n",
    "### Benchmark\n",
    "\n",
    "Our benchmark hasn't changed since step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Implement the following code pipeline using the functions you write in `lib/project_5.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/build_model.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 502)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use the function we wrote to load our data into the dataframe\n",
    "df = load_data_from_database()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Columns: 502 entries, index to label\n",
      "dtypes: int64(502)\n",
      "memory usage: 7.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Let's get info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>feat_000</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>999.500000</td>\n",
       "      <td>481.722500</td>\n",
       "      <td>483.452500</td>\n",
       "      <td>510.166000</td>\n",
       "      <td>483.384500</td>\n",
       "      <td>501.612500</td>\n",
       "      <td>479.259000</td>\n",
       "      <td>480.109500</td>\n",
       "      <td>476.565000</td>\n",
       "      <td>486.793500</td>\n",
       "      <td>...</td>\n",
       "      <td>478.811500</td>\n",
       "      <td>486.356500</td>\n",
       "      <td>496.565500</td>\n",
       "      <td>493.49950</td>\n",
       "      <td>510.893000</td>\n",
       "      <td>478.219500</td>\n",
       "      <td>483.309000</td>\n",
       "      <td>507.977000</td>\n",
       "      <td>490.266000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>577.494589</td>\n",
       "      <td>6.421769</td>\n",
       "      <td>30.186294</td>\n",
       "      <td>38.899165</td>\n",
       "      <td>9.059895</td>\n",
       "      <td>41.389418</td>\n",
       "      <td>6.795956</td>\n",
       "      <td>40.575925</td>\n",
       "      <td>1.384461</td>\n",
       "      <td>15.043836</td>\n",
       "      <td>...</td>\n",
       "      <td>4.011735</td>\n",
       "      <td>23.967366</td>\n",
       "      <td>127.635442</td>\n",
       "      <td>34.81902</td>\n",
       "      <td>37.459353</td>\n",
       "      <td>5.880613</td>\n",
       "      <td>13.559847</td>\n",
       "      <td>37.224297</td>\n",
       "      <td>25.825273</td>\n",
       "      <td>1.00025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>462.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>453.000000</td>\n",
       "      <td>371.000000</td>\n",
       "      <td>459.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>471.000000</td>\n",
       "      <td>430.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>463.000000</td>\n",
       "      <td>391.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>368.00000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>457.000000</td>\n",
       "      <td>435.000000</td>\n",
       "      <td>363.000000</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>-1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>499.750000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>464.000000</td>\n",
       "      <td>485.000000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>452.750000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>471.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>470.00000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>474.000000</td>\n",
       "      <td>474.000000</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>-1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>999.500000</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>510.500000</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>487.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>492.00000</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>478.000000</td>\n",
       "      <td>483.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1499.250000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>503.000000</td>\n",
       "      <td>536.000000</td>\n",
       "      <td>490.000000</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>484.000000</td>\n",
       "      <td>506.250000</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>496.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>481.000000</td>\n",
       "      <td>502.000000</td>\n",
       "      <td>586.000000</td>\n",
       "      <td>517.00000</td>\n",
       "      <td>535.000000</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>492.000000</td>\n",
       "      <td>533.000000</td>\n",
       "      <td>507.250000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>503.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>654.000000</td>\n",
       "      <td>519.000000</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>505.000000</td>\n",
       "      <td>611.000000</td>\n",
       "      <td>481.000000</td>\n",
       "      <td>536.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>497.000000</td>\n",
       "      <td>566.000000</td>\n",
       "      <td>920.000000</td>\n",
       "      <td>615.00000</td>\n",
       "      <td>661.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>535.000000</td>\n",
       "      <td>644.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             index     feat_000     feat_001     feat_002     feat_003  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean    999.500000   481.722500   483.452500   510.166000   483.384500   \n",
       "std     577.494589     6.421769    30.186294    38.899165     9.059895   \n",
       "min       0.000000   462.000000   381.000000   370.000000   453.000000   \n",
       "25%     499.750000   477.000000   464.000000   485.000000   477.000000   \n",
       "50%     999.500000   482.000000   483.000000   510.500000   483.000000   \n",
       "75%    1499.250000   486.000000   503.000000   536.000000   490.000000   \n",
       "max    1999.000000   503.000000   600.000000   654.000000   519.000000   \n",
       "\n",
       "          feat_004     feat_005     feat_006     feat_007     feat_008  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean    501.612500   479.259000   480.109500   476.565000   486.793500   \n",
       "std      41.389418     6.795956    40.575925     1.384461    15.043836   \n",
       "min     371.000000   459.000000   334.000000   471.000000   430.000000   \n",
       "25%     475.000000   475.000000   452.750000   476.000000   477.000000   \n",
       "50%     500.000000   479.000000   480.000000   477.000000   487.000000   \n",
       "75%     528.000000   484.000000   506.250000   477.000000   496.250000   \n",
       "max     688.000000   505.000000   611.000000   481.000000   536.000000   \n",
       "\n",
       "          ...         feat_491     feat_492     feat_493    feat_494  \\\n",
       "count     ...      2000.000000  2000.000000  2000.000000  2000.00000   \n",
       "mean      ...       478.811500   486.356500   496.565500   493.49950   \n",
       "std       ...         4.011735    23.967366   127.635442    34.81902   \n",
       "min       ...       463.000000   391.000000   130.000000   368.00000   \n",
       "25%       ...       476.000000   471.000000   404.000000   470.00000   \n",
       "50%       ...       479.000000   486.000000   504.000000   492.00000   \n",
       "75%       ...       481.000000   502.000000   586.000000   517.00000   \n",
       "max       ...       497.000000   566.000000   920.000000   615.00000   \n",
       "\n",
       "          feat_495     feat_496     feat_497     feat_498     feat_499  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean    510.893000   478.219500   483.309000   507.977000   490.266000   \n",
       "std      37.459353     5.880613    13.559847    37.224297    25.825273   \n",
       "min     398.000000   457.000000   435.000000   363.000000   403.000000   \n",
       "25%     486.000000   474.000000   474.000000   482.000000   473.000000   \n",
       "50%     511.000000   478.000000   483.000000   508.000000   490.000000   \n",
       "75%     535.000000   482.000000   492.000000   533.000000   507.250000   \n",
       "max     661.000000   500.000000   535.000000   644.000000   583.000000   \n",
       "\n",
       "            label  \n",
       "count  2000.00000  \n",
       "mean      0.00000  \n",
       "std       1.00025  \n",
       "min      -1.00000  \n",
       "25%      -1.00000  \n",
       "50%       0.00000  \n",
       "75%       1.00000  \n",
       "max       1.00000  \n",
       "\n",
       "[8 rows x 502 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the Data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test':       index  feat_000  feat_001  feat_002  feat_003  feat_004  feat_005  \\\n",
       " 1618   1618       464       463       526       487       533       481   \n",
       " 1905   1905       493       523       558       494       526       476   \n",
       " 1751   1751       479       469       504       485       482       481   \n",
       " 283     283       482       464       553       478       494       470   \n",
       " 1313   1313       474       502       508       488       593       484   \n",
       " 1803   1803       486       453       582       499       472       487   \n",
       " 1262   1262       477       510       467       488       490       493   \n",
       " 613     613       481       469       518       485       547       473   \n",
       " 1000   1000       482       538       473       485       576       479   \n",
       " 724     724       483       507       508       496       484       479   \n",
       " 1601   1601       489       457       489       475       452       475   \n",
       " 1567   1567       478       460       545       474       544       478   \n",
       " 776     776       480       515       478       487       612       474   \n",
       " 847     847       484       487       473       478       486       478   \n",
       " 1105   1105       493       556       515       486       487       471   \n",
       " 1595   1595       492       428       522       471       540       475   \n",
       " 1465   1465       488       480       485       495       502       480   \n",
       " 1983   1983       478       513       584       488       509       479   \n",
       " 1448   1448       487       486       495       481       421       481   \n",
       " 1746   1746       487       488       449       488       526       477   \n",
       " 1643   1643       476       460       510       484       563       472   \n",
       " 797     797       477       430       537       481       538       493   \n",
       " 1020   1020       485       523       540       491       381       469   \n",
       " 1005   1005       487       461       505       464       486       479   \n",
       " 1418   1418       483       502       544       479       524       497   \n",
       " 1085   1085       471       515       497       488       507       475   \n",
       " 1985   1985       483       483       523       489       422       485   \n",
       " 558     558       495       491       474       476       472       481   \n",
       " 1050   1050       489       495       543       492       449       481   \n",
       " 846     846       485       471       483       483       576       477   \n",
       " ...     ...       ...       ...       ...       ...       ...       ...   \n",
       " 1899   1899       482       499       543       479       566       478   \n",
       " 341     341       474       459       508       484       553       486   \n",
       " 432     432       493       486       517       474       607       483   \n",
       " 1173   1173       482       456       522       478       452       478   \n",
       " 450     450       486       507       470       479       487       482   \n",
       " 815     815       477       510       515       488       525       477   \n",
       " 1185   1185       486       538       589       469       533       482   \n",
       " 1702   1702       489       502       543       488       502       475   \n",
       " 80       80       481       451       508       477       526       488   \n",
       " 1850   1850       484       487       520       474       517       476   \n",
       " 1219   1219       479       480       559       481       461       486   \n",
       " 674     674       487       470       536       473       506       465   \n",
       " 214     214       490       437       505       476       508       486   \n",
       " 498     498       498       494       536       487       477       484   \n",
       " 1425   1425       487       449       552       476       512       474   \n",
       " 908     908       469       477       531       485       494       480   \n",
       " 679     679       475       483       512       474       475       484   \n",
       " 1224   1224       476       465       509       497       496       485   \n",
       " 1374   1374       470       496       443       481       511       463   \n",
       " 61       61       472       546       542       479       569       479   \n",
       " 1272   1272       470       433       439       476       515       478   \n",
       " 691     691       486       453       522       493       570       479   \n",
       " 1719   1719       480       483       478       478       507       477   \n",
       " 1468   1468       478       474       605       475       514       476   \n",
       " 892     892       488       451       491       475       548       479   \n",
       " 11       11       486       479       528       483       530       484   \n",
       " 67       67       474       497       469       490       557       480   \n",
       " 579     579       486       511       543       487       504       477   \n",
       " 1245   1245       474       499       517       485       518       473   \n",
       " 1688   1688       487       443       515       485       550       489   \n",
       " \n",
       "       feat_006  feat_007  feat_008    ...     feat_490  feat_491  feat_492  \\\n",
       " 1618       443       477       479    ...          504       478       479   \n",
       " 1905       516       475       476    ...          475       481       430   \n",
       " 1751       458       478       505    ...          524       479       482   \n",
       " 283        527       477       481    ...          495       479       525   \n",
       " 1313       495       479       475    ...          496       482       538   \n",
       " 1803       459       479       481    ...          461       483       544   \n",
       " 1262       502       477       511    ...          447       477       496   \n",
       " 613        454       475       489    ...          518       480       500   \n",
       " 1000       489       477       473    ...          450       480       482   \n",
       " 724        506       478       485    ...          478       480       504   \n",
       " 1601       476       475       494    ...          479       478       471   \n",
       " 1567       503       476       478    ...          454       483       508   \n",
       " 776        396       476       459    ...          503       483       476   \n",
       " 847        471       473       496    ...          478       472       481   \n",
       " 1105       477       477       470    ...          470       481       496   \n",
       " 1595       547       480       485    ...          486       478       495   \n",
       " 1465       552       477       491    ...          492       475       463   \n",
       " 1983       504       472       480    ...          489       483       482   \n",
       " 1448       499       478       489    ...          486       477       511   \n",
       " 1746       506       476       474    ...          473       485       492   \n",
       " 1643       431       477       486    ...          479       480       470   \n",
       " 797        426       475       492    ...          481       481       482   \n",
       " 1020       525       475       484    ...          473       481       477   \n",
       " 1005       467       478       499    ...          464       484       483   \n",
       " 1418       456       476       494    ...          489       478       473   \n",
       " 1085       503       475       482    ...          469       483       511   \n",
       " 1985       466       476       489    ...          485       475       445   \n",
       " 558        454       476       486    ...          445       482       513   \n",
       " 1050       431       476       474    ...          464       475       475   \n",
       " 846        512       478       512    ...          474       482       485   \n",
       " ...        ...       ...       ...    ...          ...       ...       ...   \n",
       " 1899       547       474       483    ...          486       481       482   \n",
       " 341        495       477       484    ...          474       479       487   \n",
       " 432        538       475       505    ...          469       483       513   \n",
       " 1173       526       481       481    ...          499       483       470   \n",
       " 450        468       476       455    ...          492       475       493   \n",
       " 815        386       476       470    ...          439       480       532   \n",
       " 1185       461       477       490    ...          513       479       493   \n",
       " 1702       486       479       467    ...          472       476       492   \n",
       " 80         420       478       491    ...          522       477       469   \n",
       " 1850       496       479       507    ...          502       479       496   \n",
       " 1219       517       476       490    ...          492       475       480   \n",
       " 674        543       477       478    ...          471       475       507   \n",
       " 214        509       478       487    ...          487       479       535   \n",
       " 498        459       477       457    ...          516       478       455   \n",
       " 1425       521       477       484    ...          493       478       480   \n",
       " 908        522       476       498    ...          450       479       496   \n",
       " 679        469       474       521    ...          489       482       474   \n",
       " 1224       491       477       505    ...          442       485       524   \n",
       " 1374       372       477       457    ...          486       474       498   \n",
       " 61         459       476       493    ...          476       480       476   \n",
       " 1272       407       477       477    ...          504       483       466   \n",
       " 691        486       475       489    ...          466       485       464   \n",
       " 1719       529       476       502    ...          475       485       458   \n",
       " 1468       427       480       492    ...          444       480       506   \n",
       " 892        468       477       489    ...          503       478       494   \n",
       " 11         479       477       462    ...          492       476       498   \n",
       " 67         462       477       478    ...          450       475       504   \n",
       " 579        503       479       478    ...          453       477       481   \n",
       " 1245       453       476       470    ...          486       479       527   \n",
       " 1688       461       474       500    ...          462       477       468   \n",
       " \n",
       "       feat_493  feat_494  feat_495  feat_496  feat_497  feat_498  feat_499  \n",
       " 1618       492       500       533       469       459       517       518  \n",
       " 1905       267       495       550       492       490       503       469  \n",
       " 1751       444       431       514       476       496       543       464  \n",
       " 283        637       451       557       475       479       551       481  \n",
       " 1313       531       564       553       491       459       454       510  \n",
       " 1803       507       464       498       475       488       484       513  \n",
       " 1262       587       528       589       478       489       508       454  \n",
       " 613        379       542       503       477       508       574       448  \n",
       " 1000       382       522       486       475       497       487       439  \n",
       " 724        438       493       552       483       501       515       473  \n",
       " 1601       336       459       508       470       499       521       494  \n",
       " 1567       320       549       520       478       493       579       483  \n",
       " 776        509       532       487       477       469       548       514  \n",
       " 847        559       484       540       485       498       575       498  \n",
       " 1105       423       493       501       483       506       477       508  \n",
       " 1595       606       485       424       477       471       539       496  \n",
       " 1465       757       563       458       475       474       545       535  \n",
       " 1983       458       485       474       487       488       465       495  \n",
       " 1448       245       522       480       483       493       421       488  \n",
       " 1746       318       421       507       474       467       526       507  \n",
       " 1643       528       463       436       485       492       576       470  \n",
       " 797        439       527       495       478       470       472       409  \n",
       " 1020       589       478       501       472       487       476       428  \n",
       " 1005       394       472       548       469       479       527       486  \n",
       " 1418       742       475       500       473       481       542       515  \n",
       " 1085       286       479       484       486       468       520       494  \n",
       " 1985       600       488       515       479       470       500       451  \n",
       " 558        421       496       580       474       479       437       504  \n",
       " 1050       665       507       451       493       470       500       478  \n",
       " 846        474       475       485       480       486       472       465  \n",
       " ...        ...       ...       ...       ...       ...       ...       ...  \n",
       " 1899       424       456       546       480       473       547       509  \n",
       " 341        636       515       478       479       507       491       508  \n",
       " 432        405       473       548       483       471       522       444  \n",
       " 1173       577       466       458       479       492       475       509  \n",
       " 450        367       520       553       482       484       567       480  \n",
       " 815        737       475       457       473       475       565       470  \n",
       " 1185       589       460       505       487       503       524       465  \n",
       " 1702       423       531       494       479       457       509       519  \n",
       " 80         488       520       512       474       501       527       497  \n",
       " 1850       360       525       566       480       490       530       486  \n",
       " 1219       332       524       504       479       497       528       480  \n",
       " 674        438       523       442       487       485       489       523  \n",
       " 214        601       542       496       479       475       545       463  \n",
       " 498        714       440       519       473       477       503       511  \n",
       " 1425       499       495       608       473       497       495       478  \n",
       " 908        414       533       479       470       472       500       453  \n",
       " 679        580       516       497       477       459       482       511  \n",
       " 1224       455       502       500       472       511       518       500  \n",
       " 1374       330       509       526       487       502       486       493  \n",
       " 61         618       501       582       487       487       479       488  \n",
       " 1272       225       480       505       479       482       523       510  \n",
       " 691        489       516       458       478       464       498       537  \n",
       " 1719       240       542       431       482       469       522       510  \n",
       " 1468       474       526       493       480       483       552       529  \n",
       " 892        306       498       547       478       476       483       473  \n",
       " 11         599       459       452       475       496       505       485  \n",
       " 67         174       492       518       476       481       561       452  \n",
       " 579        473       517       491       478       497       500       503  \n",
       " 1245       417       470       473       489       492       426       465  \n",
       " 1688       534       552       504       476       488       522       495  \n",
       " \n",
       " [500 rows x 501 columns],\n",
       " 'X_train':       index  feat_000  feat_001  feat_002  feat_003  feat_004  feat_005  \\\n",
       " 1891   1891       478       515       534       482       497       477   \n",
       " 1591   1591       483       449       446       473       539       477   \n",
       " 408     408       485       443       465       472       532       478   \n",
       " 701     701       479       467       543       497       515       473   \n",
       " 609     609       483       488       491       471       510       476   \n",
       " 713     713       483       493       514       477       510       489   \n",
       " 1712   1712       479       457       485       475       572       473   \n",
       " 1930   1930       491       484       581       502       534       490   \n",
       " 1195   1195       491       535       517       483       480       477   \n",
       " 1133   1133       482       504       447       466       572       489   \n",
       " 571     571       487       501       451       470       540       481   \n",
       " 1459   1459       484       517       562       489       526       477   \n",
       " 1863   1863       473       482       438       484       511       478   \n",
       " 1685   1685       476       491       477       485       498       470   \n",
       " 835     835       484       503       525       485       527       477   \n",
       " 1481   1481       472       498       484       492       519       488   \n",
       " 1813   1813       475       471       562       474       495       483   \n",
       " 154     154       472       439       511       478       446       479   \n",
       " 1215   1215       475       465       527       468       452       480   \n",
       " 1064   1064       475       450       534       490       524       478   \n",
       " 518     518       482       507       510       474       476       472   \n",
       " 1525   1525       478       482       515       473       442       479   \n",
       " 393     393       483       408       514       488       528       488   \n",
       " 1010   1010       480       440       502       477       527       476   \n",
       " 1175   1175       485       484       521       488       473       475   \n",
       " 868     868       482       527       551       493       580       480   \n",
       " 714     714       483       460       506       480       536       475   \n",
       " 481     481       481       460       515       470       485       479   \n",
       " 551     551       484       496       518       477       563       477   \n",
       " 1739   1739       479       472       445       466       476       474   \n",
       " ...     ...       ...       ...       ...       ...       ...       ...   \n",
       " 291     291       495       478       488       469       523       482   \n",
       " 1003   1003       481       483       535       465       509       484   \n",
       " 1843   1843       478       491       508       488       488       469   \n",
       " 1695   1695       483       473       508       485       529       477   \n",
       " 1068   1068       480       496       556       481       493       485   \n",
       " 1090   1090       480       459       562       479       441       481   \n",
       " 803     803       474       511       492       468       496       479   \n",
       " 1555   1555       487       476       563       506       505       488   \n",
       " 925     925       480       506       525       481       554       474   \n",
       " 1610   1610       477       516       510       506       510       475   \n",
       " 1146   1146       482       464       487       481       479       476   \n",
       " 1608   1608       483       501       528       499       560       468   \n",
       " 1632   1632       476       512       478       484       546       479   \n",
       " 1470   1470       476       477       509       466       549       477   \n",
       " 1305   1305       487       480       524       487       488       479   \n",
       " 1637   1637       477       510       581       483       535       478   \n",
       " 1406   1406       478       498       540       481       543       476   \n",
       " 1837   1837       475       489       505       495       538       479   \n",
       " 353     353       480       463       553       479       516       485   \n",
       " 1882   1882       482       530       514       489       549       482   \n",
       " 660     660       473       452       566       476       445       490   \n",
       " 509     509       475       477       486       462       551       476   \n",
       " 1575   1575       477       544       504       490       549       470   \n",
       " 200     200       486       474       464       484       466       473   \n",
       " 441     441       484       475       483       482       521       479   \n",
       " 1197   1197       483       452       477       467       493       481   \n",
       " 537     537       482       508       500       482       447       483   \n",
       " 1798   1798       468       468       451       478       574       478   \n",
       " 764     764       486       487       527       477       502       477   \n",
       " 885     885       489       518       547       476       554       485   \n",
       " \n",
       "       feat_006  feat_007  feat_008    ...     feat_490  feat_491  feat_492  \\\n",
       " 1891       406       475       475    ...          485       481       501   \n",
       " 1591       449       476       495    ...          466       481       454   \n",
       " 408        497       474       492    ...          503       479       504   \n",
       " 701        422       473       482    ...          442       478       485   \n",
       " 609        486       478       509    ...          460       475       492   \n",
       " 713        495       476       498    ...          488       478       503   \n",
       " 1712       496       476       513    ...          480       476       441   \n",
       " 1930       477       477       477    ...          444       472       487   \n",
       " 1195       490       475       512    ...          463       477       504   \n",
       " 1133       440       475       466    ...          522       480       487   \n",
       " 571        453       478       467    ...          491       474       489   \n",
       " 1459       444       476       517    ...          536       482       458   \n",
       " 1863       519       474       501    ...          464       481       467   \n",
       " 1685       508       478       503    ...          460       480       507   \n",
       " 835        507       477       479    ...          485       474       503   \n",
       " 1481       440       477       479    ...          516       485       497   \n",
       " 1813       456       477       497    ...          473       470       523   \n",
       " 154        531       478       496    ...          472       479       496   \n",
       " 1215       469       474       521    ...          461       484       520   \n",
       " 1064       479       475       485    ...          513       485       464   \n",
       " 518        487       475       519    ...          528       475       485   \n",
       " 1525       425       477       462    ...          474       479       488   \n",
       " 393        416       476       491    ...          509       479       477   \n",
       " 1010       457       477       495    ...          456       472       491   \n",
       " 1175       492       475       483    ...          457       479       525   \n",
       " 868        497       475       496    ...          468       482       504   \n",
       " 714        400       475       474    ...          462       485       495   \n",
       " 481        526       477       489    ...          538       481       480   \n",
       " 551        509       477       482    ...          488       484       466   \n",
       " 1739       498       475       483    ...          481       474       513   \n",
       " ...        ...       ...       ...    ...          ...       ...       ...   \n",
       " 291        491       477       486    ...          481       482       517   \n",
       " 1003       449       476       499    ...          503       484       466   \n",
       " 1843       440       476       506    ...          496       480       462   \n",
       " 1695       458       476       470    ...          496       483       488   \n",
       " 1068       547       475       472    ...          490       479       466   \n",
       " 1090       462       475       507    ...          525       479       464   \n",
       " 803        452       475       474    ...          467       483       501   \n",
       " 1555       505       477       502    ...          491       476       504   \n",
       " 925        433       475       484    ...          472       485       517   \n",
       " 1610       611       478       500    ...          480       470       491   \n",
       " 1146       546       477       505    ...          520       481       498   \n",
       " 1608       452       479       471    ...          458       478       462   \n",
       " 1632       492       476       486    ...          474       483       446   \n",
       " 1470       477       478       511    ...          525       480       464   \n",
       " 1305       494       478       484    ...          483       475       492   \n",
       " 1637       550       475       484    ...          468       479       500   \n",
       " 1406       516       476       469    ...          510       490       519   \n",
       " 1837       394       477       498    ...          464       479       479   \n",
       " 353        494       474       493    ...          451       477       427   \n",
       " 1882       391       478       493    ...          476       477       512   \n",
       " 660        446       478       468    ...          518       476       449   \n",
       " 509        473       478       509    ...          481       477       498   \n",
       " 1575       430       474       500    ...          489       472       466   \n",
       " 200        377       478       508    ...          507       484       483   \n",
       " 441        451       476       506    ...          525       477       452   \n",
       " 1197       445       476       486    ...          497       481       475   \n",
       " 537        446       475       483    ...          473       480       506   \n",
       " 1798       501       478       488    ...          457       475       480   \n",
       " 764        503       476       467    ...          503       482       468   \n",
       " 885        465       475       484    ...          492       480       530   \n",
       " \n",
       "       feat_493  feat_494  feat_495  feat_496  feat_497  feat_498  feat_499  \n",
       " 1891       624       527       544       483       481       490       451  \n",
       " 1591       439       490       562       470       475       464       453  \n",
       " 408        384       533       491       478       491       496       477  \n",
       " 701        370       470       493       476       483       457       502  \n",
       " 609        332       515       485       477       473       484       509  \n",
       " 713        737       460       474       468       487       535       482  \n",
       " 1712       668       513       495       478       453       553       541  \n",
       " 1930       611       516       550       480       496       521       439  \n",
       " 1195       566       513       458       485       477       477       506  \n",
       " 1133       523       518       514       474       472       498       482  \n",
       " 571        468       502       465       491       472       553       491  \n",
       " 1459       382       516       533       479       510       478       510  \n",
       " 1863       478       463       571       485       470       524       482  \n",
       " 1685       751       430       502       472       490       518       493  \n",
       " 835        389       485       516       473       483       519       494  \n",
       " 1481       611       453       490       476       487       528       523  \n",
       " 1813       425       446       525       470       484       534       494  \n",
       " 154        665       543       547       475       485       472       523  \n",
       " 1215       224       474       567       479       475       490       495  \n",
       " 1064       538       499       535       475       494       478       485  \n",
       " 518        702       499       515       478       470       469       473  \n",
       " 1525       504       479       496       471       468       505       518  \n",
       " 393        464       458       523       480       474       508       444  \n",
       " 1010       489       475       523       482       494       552       483  \n",
       " 1175       635       503       479       480       473       525       541  \n",
       " 868        433       474       457       469       478       523       520  \n",
       " 714        494       487       462       476       463       519       491  \n",
       " 481        477       454       470       485       500       465       524  \n",
       " 551        387       488       548       477       489       535       506  \n",
       " 1739       421       483       509       481       490       537       445  \n",
       " ...        ...       ...       ...       ...       ...       ...       ...  \n",
       " 291        673       480       494       462       486       527       460  \n",
       " 1003       534       537       585       485       466       498       452  \n",
       " 1843       510       489       501       479       468       435       462  \n",
       " 1695       413       487       521       461       468       490       427  \n",
       " 1068       365       466       510       489       507       470       497  \n",
       " 1090       458       557       518       476       473       439       490  \n",
       " 803        213       551       563       478       468       521       491  \n",
       " 1555       596       527       544       470       469       525       492  \n",
       " 925        453       546       444       473       489       519       485  \n",
       " 1610       347       491       484       477       453       406       497  \n",
       " 1146       437       497       482       479       495       566       466  \n",
       " 1608       471       507       555       473       471       573       472  \n",
       " 1632       575       491       514       479       479       517       508  \n",
       " 1470       566       538       520       479       493       528       521  \n",
       " 1305       619       460       501       479       474       456       463  \n",
       " 1637       584       511       528       478       477       477       479  \n",
       " 1406       484       489       426       472       504       495       456  \n",
       " 1837       363       484       514       482       467       496       473  \n",
       " 353        435       510       554       478       483       505       502  \n",
       " 1882       600       421       492       469       483       595       491  \n",
       " 660        458       471       520       478       481       576       538  \n",
       " 509        323       523       503       476       496       494       487  \n",
       " 1575       457       480       529       478       503       491       507  \n",
       " 200        584       435       471       482       504       526       483  \n",
       " 441        529       490       462       484       487       520       492  \n",
       " 1197       539       526       521       477       471       503       531  \n",
       " 537        502       513       511       482       483       513       513  \n",
       " 1798       477       537       557       476       477       509       461  \n",
       " 764        583       505       509       482       479       499       515  \n",
       " 885        571       421       513       481       482       539       487  \n",
       " \n",
       " [1500 rows x 501 columns],\n",
       " 'y_test': 1618   -1\n",
       " 1905    1\n",
       " 1751    1\n",
       " 283     1\n",
       " 1313   -1\n",
       " 1803   -1\n",
       " 1262    1\n",
       " 613     1\n",
       " 1000   -1\n",
       " 724    -1\n",
       " 1601    1\n",
       " 1567    1\n",
       " 776    -1\n",
       " 847     1\n",
       " 1105    1\n",
       " 1595    1\n",
       " 1465    1\n",
       " 1983   -1\n",
       " 1448    1\n",
       " 1746    1\n",
       " 1643    1\n",
       " 797    -1\n",
       " 1020    1\n",
       " 1005   -1\n",
       " 1418   -1\n",
       " 1085   -1\n",
       " 1985    1\n",
       " 558    -1\n",
       " 1050   -1\n",
       " 846    -1\n",
       "        ..\n",
       " 1899    1\n",
       " 341     1\n",
       " 432    -1\n",
       " 1173    1\n",
       " 450     1\n",
       " 815     1\n",
       " 1185    1\n",
       " 1702    1\n",
       " 80     -1\n",
       " 1850   -1\n",
       " 1219    1\n",
       " 674    -1\n",
       " 214    -1\n",
       " 498    -1\n",
       " 1425   -1\n",
       " 908     1\n",
       " 679     1\n",
       " 1224    1\n",
       " 1374    1\n",
       " 61      1\n",
       " 1272    1\n",
       " 691    -1\n",
       " 1719    1\n",
       " 1468    1\n",
       " 892    -1\n",
       " 11     -1\n",
       " 67      1\n",
       " 579    -1\n",
       " 1245   -1\n",
       " 1688   -1\n",
       " Name: label, dtype: int64,\n",
       " 'y_train': 1891   -1\n",
       " 1591    1\n",
       " 408    -1\n",
       " 701     1\n",
       " 609    -1\n",
       " 713     1\n",
       " 1712   -1\n",
       " 1930   -1\n",
       " 1195   -1\n",
       " 1133    1\n",
       " 571    -1\n",
       " 1459   -1\n",
       " 1863    1\n",
       " 1685   -1\n",
       " 835     1\n",
       " 1481    1\n",
       " 1813   -1\n",
       " 154    -1\n",
       " 1215   -1\n",
       " 1064   -1\n",
       " 518    -1\n",
       " 1525    1\n",
       " 393     1\n",
       " 1010   -1\n",
       " 1175   -1\n",
       " 868    -1\n",
       " 714    -1\n",
       " 481    -1\n",
       " 551     1\n",
       " 1739   -1\n",
       "        ..\n",
       " 291     1\n",
       " 1003    1\n",
       " 1843    1\n",
       " 1695    1\n",
       " 1068    1\n",
       " 1090   -1\n",
       " 803     1\n",
       " 1555    1\n",
       " 925     1\n",
       " 1610   -1\n",
       " 1146   -1\n",
       " 1608    1\n",
       " 1632    1\n",
       " 1470    1\n",
       " 1305   -1\n",
       " 1637    1\n",
       " 1406    1\n",
       " 1837    1\n",
       " 353    -1\n",
       " 1882    1\n",
       " 660     1\n",
       " 509     1\n",
       " 1575   -1\n",
       " 200    -1\n",
       " 441     1\n",
       " 1197    1\n",
       " 537    -1\n",
       " 1798   -1\n",
       " 764    -1\n",
       " 885    -1\n",
       " Name: label, dtype: int64}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load our make_data_dict function to \n",
    "df = make_data_dict(df, random_state=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import samples_generator\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[ 1.05367159, -2.77190284, -0.6814072 , ..., -1.80218292,\n",
       "          0.23585583,  1.07256347],\n",
       "        [ 1.54990462,  1.77014611,  1.32233873, ...,  0.50468266,\n",
       "         -0.13487564, -0.82584753],\n",
       "        [ 1.28363323, -0.42256717, -0.4810326 , ...,  0.95117277,\n",
       "          0.92435712, -1.01956294],\n",
       "        ..., \n",
       "        [-0.74279572,  0.67378947,  0.92158954, ...,  1.02558779,\n",
       "         -0.21431809,  0.49141725],\n",
       "        [ 0.40874155, -1.20567906,  0.52084036, ...,  0.65351269,\n",
       "         -2.1738987 , -0.98081986],\n",
       "        [ 1.17470403,  0.83041185, -1.3493225 , ...,  0.35585262,\n",
       "          0.36825992,  0.18147259]]),\n",
       " 'X_train': array([[ 1.52569813, -0.57918955,  1.0551726 , ..., -0.16505251,\n",
       "         -0.47912628, -1.523223  ],\n",
       "        [ 1.00698764,  0.20392234, -1.14894791, ..., -0.61154262,\n",
       "         -1.16762758, -1.44573684],\n",
       "        [-1.03846069,  0.51716709, -1.3493225 , ...,  0.57909768,\n",
       "         -0.32024137, -0.51590288],\n",
       "        ..., \n",
       "        [ 1.36489788, -2.14541333, -0.51442837, ..., -0.46271259,\n",
       "          0.02400928, -1.13579218],\n",
       "        [-0.42292425,  0.67378947,  0.12009117, ..., -0.31388255,\n",
       "         -0.24079891,  0.95633423],\n",
       "        [-0.21371102,  1.1436566 ,  1.1553599 , ..., -0.09063749,\n",
       "          0.81843384, -0.12847206]]),\n",
       " 'processes': [StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  SelectKBest(k='all', score_func=<function f_classif at 0x1149729b0>),\n",
       "  Pipeline(steps=[('selectkbest', SelectKBest(k=5, score_func=<function f_classif at 0x1149729b0>)), ('kneighborsclassifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n",
       "             weights='uniform'))]),\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True)],\n",
       " 'test_score': 0.64800000000000002,\n",
       " 'train_score': 0.70199999999999996,\n",
       " 'y_test': 1618   -1\n",
       " 1905    1\n",
       " 1751    1\n",
       " 283     1\n",
       " 1313   -1\n",
       " 1803   -1\n",
       " 1262    1\n",
       " 613     1\n",
       " 1000   -1\n",
       " 724    -1\n",
       " 1601    1\n",
       " 1567    1\n",
       " 776    -1\n",
       " 847     1\n",
       " 1105    1\n",
       " 1595    1\n",
       " 1465    1\n",
       " 1983   -1\n",
       " 1448    1\n",
       " 1746    1\n",
       " 1643    1\n",
       " 797    -1\n",
       " 1020    1\n",
       " 1005   -1\n",
       " 1418   -1\n",
       " 1085   -1\n",
       " 1985    1\n",
       " 558    -1\n",
       " 1050   -1\n",
       " 846    -1\n",
       "        ..\n",
       " 1899    1\n",
       " 341     1\n",
       " 432    -1\n",
       " 1173    1\n",
       " 450     1\n",
       " 815     1\n",
       " 1185    1\n",
       " 1702    1\n",
       " 80     -1\n",
       " 1850   -1\n",
       " 1219    1\n",
       " 674    -1\n",
       " 214    -1\n",
       " 498    -1\n",
       " 1425   -1\n",
       " 908     1\n",
       " 679     1\n",
       " 1224    1\n",
       " 1374    1\n",
       " 61      1\n",
       " 1272    1\n",
       " 691    -1\n",
       " 1719    1\n",
       " 1468    1\n",
       " 892    -1\n",
       " 11     -1\n",
       " 67      1\n",
       " 579    -1\n",
       " 1245   -1\n",
       " 1688   -1\n",
       " Name: label, dtype: int64,\n",
       " 'y_train': 1891   -1\n",
       " 1591    1\n",
       " 408    -1\n",
       " 701     1\n",
       " 609    -1\n",
       " 713     1\n",
       " 1712   -1\n",
       " 1930   -1\n",
       " 1195   -1\n",
       " 1133    1\n",
       " 571    -1\n",
       " 1459   -1\n",
       " 1863    1\n",
       " 1685   -1\n",
       " 835     1\n",
       " 1481    1\n",
       " 1813   -1\n",
       " 154    -1\n",
       " 1215   -1\n",
       " 1064   -1\n",
       " 518    -1\n",
       " 1525    1\n",
       " 393     1\n",
       " 1010   -1\n",
       " 1175   -1\n",
       " 868    -1\n",
       " 714    -1\n",
       " 481    -1\n",
       " 551     1\n",
       " 1739   -1\n",
       "        ..\n",
       " 291     1\n",
       " 1003    1\n",
       " 1843    1\n",
       " 1695    1\n",
       " 1068    1\n",
       " 1090   -1\n",
       " 803     1\n",
       " 1555    1\n",
       " 925     1\n",
       " 1610   -1\n",
       " 1146   -1\n",
       " 1608    1\n",
       " 1632    1\n",
       " 1470    1\n",
       " 1305   -1\n",
       " 1637    1\n",
       " 1406    1\n",
       " 1837    1\n",
       " 353    -1\n",
       " 1882    1\n",
       " 660     1\n",
       " 509     1\n",
       " 1575   -1\n",
       " 200    -1\n",
       " 441     1\n",
       " 1197    1\n",
       " 537    -1\n",
       " 1798   -1\n",
       " 764    -1\n",
       " 885    -1\n",
       " Name: label, dtype: int64}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will scale our data with Standard Scaler\n",
    "df = general_transformer(StandardScaler(), df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct a Pipeline that uses SelectKBest to transform data\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "df = general_transformer(SelectKBest(f_classif, k='all'), df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[ 1.05367159, -2.77190284, -0.6814072 , ..., -1.80218292,\n",
       "          0.23585583,  1.07256347],\n",
       "        [ 1.54990462,  1.77014611,  1.32233873, ...,  0.50468266,\n",
       "         -0.13487564, -0.82584753],\n",
       "        [ 1.28363323, -0.42256717, -0.4810326 , ...,  0.95117277,\n",
       "          0.92435712, -1.01956294],\n",
       "        ..., \n",
       "        [-0.74279572,  0.67378947,  0.92158954, ...,  1.02558779,\n",
       "         -0.21431809,  0.49141725],\n",
       "        [ 0.40874155, -1.20567906,  0.52084036, ...,  0.65351269,\n",
       "         -2.1738987 , -0.98081986],\n",
       "        [ 1.17470403,  0.83041185, -1.3493225 , ...,  0.35585262,\n",
       "          0.36825992,  0.18147259]]),\n",
       " 'X_train': array([[ 1.52569813, -0.57918955,  1.0551726 , ..., -0.16505251,\n",
       "         -0.47912628, -1.523223  ],\n",
       "        [ 1.00698764,  0.20392234, -1.14894791, ..., -0.61154262,\n",
       "         -1.16762758, -1.44573684],\n",
       "        [-1.03846069,  0.51716709, -1.3493225 , ...,  0.57909768,\n",
       "         -0.32024137, -0.51590288],\n",
       "        ..., \n",
       "        [ 1.36489788, -2.14541333, -0.51442837, ..., -0.46271259,\n",
       "          0.02400928, -1.13579218],\n",
       "        [-0.42292425,  0.67378947,  0.12009117, ..., -0.31388255,\n",
       "         -0.24079891,  0.95633423],\n",
       "        [-0.21371102,  1.1436566 ,  1.1553599 , ..., -0.09063749,\n",
       "          0.81843384, -0.12847206]]),\n",
       " 'processes': [StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  SelectKBest(k='all', score_func=<function f_classif at 0x1149729b0>),\n",
       "  Pipeline(steps=[('selectkbest', SelectKBest(k=5, score_func=<function f_classif at 0x1149729b0>)), ('kneighborsclassifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n",
       "             weights='uniform'))]),\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  SelectKBest(k='all', score_func=<function f_classif at 0x1149729b0>)],\n",
       " 'test_score': 0.64800000000000002,\n",
       " 'train_score': 0.70199999999999996,\n",
       " 'y_test': 1618   -1\n",
       " 1905    1\n",
       " 1751    1\n",
       " 283     1\n",
       " 1313   -1\n",
       " 1803   -1\n",
       " 1262    1\n",
       " 613     1\n",
       " 1000   -1\n",
       " 724    -1\n",
       " 1601    1\n",
       " 1567    1\n",
       " 776    -1\n",
       " 847     1\n",
       " 1105    1\n",
       " 1595    1\n",
       " 1465    1\n",
       " 1983   -1\n",
       " 1448    1\n",
       " 1746    1\n",
       " 1643    1\n",
       " 797    -1\n",
       " 1020    1\n",
       " 1005   -1\n",
       " 1418   -1\n",
       " 1085   -1\n",
       " 1985    1\n",
       " 558    -1\n",
       " 1050   -1\n",
       " 846    -1\n",
       "        ..\n",
       " 1899    1\n",
       " 341     1\n",
       " 432    -1\n",
       " 1173    1\n",
       " 450     1\n",
       " 815     1\n",
       " 1185    1\n",
       " 1702    1\n",
       " 80     -1\n",
       " 1850   -1\n",
       " 1219    1\n",
       " 674    -1\n",
       " 214    -1\n",
       " 498    -1\n",
       " 1425   -1\n",
       " 908     1\n",
       " 679     1\n",
       " 1224    1\n",
       " 1374    1\n",
       " 61      1\n",
       " 1272    1\n",
       " 691    -1\n",
       " 1719    1\n",
       " 1468    1\n",
       " 892    -1\n",
       " 11     -1\n",
       " 67      1\n",
       " 579    -1\n",
       " 1245   -1\n",
       " 1688   -1\n",
       " Name: label, dtype: int64,\n",
       " 'y_train': 1891   -1\n",
       " 1591    1\n",
       " 408    -1\n",
       " 701     1\n",
       " 609    -1\n",
       " 713     1\n",
       " 1712   -1\n",
       " 1930   -1\n",
       " 1195   -1\n",
       " 1133    1\n",
       " 571    -1\n",
       " 1459   -1\n",
       " 1863    1\n",
       " 1685   -1\n",
       " 835     1\n",
       " 1481    1\n",
       " 1813   -1\n",
       " 154    -1\n",
       " 1215   -1\n",
       " 1064   -1\n",
       " 518    -1\n",
       " 1525    1\n",
       " 393     1\n",
       " 1010   -1\n",
       " 1175   -1\n",
       " 868    -1\n",
       " 714    -1\n",
       " 481    -1\n",
       " 551     1\n",
       " 1739   -1\n",
       "        ..\n",
       " 291     1\n",
       " 1003    1\n",
       " 1843    1\n",
       " 1695    1\n",
       " 1068    1\n",
       " 1090   -1\n",
       " 803     1\n",
       " 1555    1\n",
       " 925     1\n",
       " 1610   -1\n",
       " 1146   -1\n",
       " 1608    1\n",
       " 1632    1\n",
       " 1470    1\n",
       " 1305   -1\n",
       " 1637    1\n",
       " 1406    1\n",
       " 1837    1\n",
       " 353    -1\n",
       " 1882    1\n",
       " 660     1\n",
       " 509     1\n",
       " 1575   -1\n",
       " 200    -1\n",
       " 441     1\n",
       " 1197    1\n",
       " 537    -1\n",
       " 1798   -1\n",
       " 764    -1\n",
       " 885    -1\n",
       " Name: label, dtype: int64}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[ 1.05367159, -2.77190284, -0.6814072 , ..., -1.80218292,\n",
       "          0.23585583,  1.07256347],\n",
       "        [ 1.54990462,  1.77014611,  1.32233873, ...,  0.50468266,\n",
       "         -0.13487564, -0.82584753],\n",
       "        [ 1.28363323, -0.42256717, -0.4810326 , ...,  0.95117277,\n",
       "          0.92435712, -1.01956294],\n",
       "        ..., \n",
       "        [-0.74279572,  0.67378947,  0.92158954, ...,  1.02558779,\n",
       "         -0.21431809,  0.49141725],\n",
       "        [ 0.40874155, -1.20567906,  0.52084036, ...,  0.65351269,\n",
       "         -2.1738987 , -0.98081986],\n",
       "        [ 1.17470403,  0.83041185, -1.3493225 , ...,  0.35585262,\n",
       "          0.36825992,  0.18147259]]),\n",
       " 'X_train': array([[ 1.52569813, -0.57918955,  1.0551726 , ..., -0.16505251,\n",
       "         -0.47912628, -1.523223  ],\n",
       "        [ 1.00698764,  0.20392234, -1.14894791, ..., -0.61154262,\n",
       "         -1.16762758, -1.44573684],\n",
       "        [-1.03846069,  0.51716709, -1.3493225 , ...,  0.57909768,\n",
       "         -0.32024137, -0.51590288],\n",
       "        ..., \n",
       "        [ 1.36489788, -2.14541333, -0.51442837, ..., -0.46271259,\n",
       "          0.02400928, -1.13579218],\n",
       "        [-0.42292425,  0.67378947,  0.12009117, ..., -0.31388255,\n",
       "         -0.24079891,  0.95633423],\n",
       "        [-0.21371102,  1.1436566 ,  1.1553599 , ..., -0.09063749,\n",
       "          0.81843384, -0.12847206]]),\n",
       " 'processes': [StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  SelectKBest(k='all', score_func=<function f_classif at 0x1149729b0>),\n",
       "  Pipeline(steps=[('selectkbest', SelectKBest(k=5, score_func=<function f_classif at 0x1149729b0>)), ('kneighborsclassifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n",
       "             weights='uniform'))]),\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  SelectKBest(k='all', score_func=<function f_classif at 0x1149729b0>),\n",
       "  Pipeline(steps=[('selectkbest', SelectKBest(k=5, score_func=<function f_classif at 0x1149729b0>)), ('kneighborsclassifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n",
       "             weights='uniform'))]),\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "            solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False)],\n",
       " 'test_score': 0.504,\n",
       " 'train_score': 0.79533333333333334,\n",
       " 'y_test': 1618   -1\n",
       " 1905    1\n",
       " 1751    1\n",
       " 283     1\n",
       " 1313   -1\n",
       " 1803   -1\n",
       " 1262    1\n",
       " 613     1\n",
       " 1000   -1\n",
       " 724    -1\n",
       " 1601    1\n",
       " 1567    1\n",
       " 776    -1\n",
       " 847     1\n",
       " 1105    1\n",
       " 1595    1\n",
       " 1465    1\n",
       " 1983   -1\n",
       " 1448    1\n",
       " 1746    1\n",
       " 1643    1\n",
       " 797    -1\n",
       " 1020    1\n",
       " 1005   -1\n",
       " 1418   -1\n",
       " 1085   -1\n",
       " 1985    1\n",
       " 558    -1\n",
       " 1050   -1\n",
       " 846    -1\n",
       "        ..\n",
       " 1899    1\n",
       " 341     1\n",
       " 432    -1\n",
       " 1173    1\n",
       " 450     1\n",
       " 815     1\n",
       " 1185    1\n",
       " 1702    1\n",
       " 80     -1\n",
       " 1850   -1\n",
       " 1219    1\n",
       " 674    -1\n",
       " 214    -1\n",
       " 498    -1\n",
       " 1425   -1\n",
       " 908     1\n",
       " 679     1\n",
       " 1224    1\n",
       " 1374    1\n",
       " 61      1\n",
       " 1272    1\n",
       " 691    -1\n",
       " 1719    1\n",
       " 1468    1\n",
       " 892    -1\n",
       " 11     -1\n",
       " 67      1\n",
       " 579    -1\n",
       " 1245   -1\n",
       " 1688   -1\n",
       " Name: label, dtype: int64,\n",
       " 'y_train': 1891   -1\n",
       " 1591    1\n",
       " 408    -1\n",
       " 701     1\n",
       " 609    -1\n",
       " 713     1\n",
       " 1712   -1\n",
       " 1930   -1\n",
       " 1195   -1\n",
       " 1133    1\n",
       " 571    -1\n",
       " 1459   -1\n",
       " 1863    1\n",
       " 1685   -1\n",
       " 835     1\n",
       " 1481    1\n",
       " 1813   -1\n",
       " 154    -1\n",
       " 1215   -1\n",
       " 1064   -1\n",
       " 518    -1\n",
       " 1525    1\n",
       " 393     1\n",
       " 1010   -1\n",
       " 1175   -1\n",
       " 868    -1\n",
       " 714    -1\n",
       " 481    -1\n",
       " 551     1\n",
       " 1739   -1\n",
       "        ..\n",
       " 291     1\n",
       " 1003    1\n",
       " 1843    1\n",
       " 1695    1\n",
       " 1068    1\n",
       " 1090   -1\n",
       " 803     1\n",
       " 1555    1\n",
       " 925     1\n",
       " 1610   -1\n",
       " 1146   -1\n",
       " 1608    1\n",
       " 1632    1\n",
       " 1470    1\n",
       " 1305   -1\n",
       " 1637    1\n",
       " 1406    1\n",
       " 1837    1\n",
       " 353    -1\n",
       " 1882    1\n",
       " 660     1\n",
       " 509     1\n",
       " 1575   -1\n",
       " 200    -1\n",
       " 441     1\n",
       " 1197    1\n",
       " 537    -1\n",
       " 1798   -1\n",
       " 764    -1\n",
       " 885    -1\n",
       " Name: label, dtype: int64}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running a general logistic regression with LASSO as the penalty\n",
    "df = general_model(LogisticRegression(C=1, penalty='l1'), df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constructing a pipeline\n",
    "this_pipeline = (SelectKBest(f_classif, k=5),\n",
    "                 KNeighborsClassifier(n_neighbors=17))\n",
    "anova_classifier = make_pipeline(*this_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[ 1.05367159, -2.77190284, -0.6814072 , ..., -1.80218292,\n",
       "          0.23585583,  1.07256347],\n",
       "        [ 1.54990462,  1.77014611,  1.32233873, ...,  0.50468266,\n",
       "         -0.13487564, -0.82584753],\n",
       "        [ 1.28363323, -0.42256717, -0.4810326 , ...,  0.95117277,\n",
       "          0.92435712, -1.01956294],\n",
       "        ..., \n",
       "        [-0.74279572,  0.67378947,  0.92158954, ...,  1.02558779,\n",
       "         -0.21431809,  0.49141725],\n",
       "        [ 0.40874155, -1.20567906,  0.52084036, ...,  0.65351269,\n",
       "         -2.1738987 , -0.98081986],\n",
       "        [ 1.17470403,  0.83041185, -1.3493225 , ...,  0.35585262,\n",
       "          0.36825992,  0.18147259]]),\n",
       " 'X_train': array([[ 1.52569813, -0.57918955,  1.0551726 , ..., -0.16505251,\n",
       "         -0.47912628, -1.523223  ],\n",
       "        [ 1.00698764,  0.20392234, -1.14894791, ..., -0.61154262,\n",
       "         -1.16762758, -1.44573684],\n",
       "        [-1.03846069,  0.51716709, -1.3493225 , ...,  0.57909768,\n",
       "         -0.32024137, -0.51590288],\n",
       "        ..., \n",
       "        [ 1.36489788, -2.14541333, -0.51442837, ..., -0.46271259,\n",
       "          0.02400928, -1.13579218],\n",
       "        [-0.42292425,  0.67378947,  0.12009117, ..., -0.31388255,\n",
       "         -0.24079891,  0.95633423],\n",
       "        [-0.21371102,  1.1436566 ,  1.1553599 , ..., -0.09063749,\n",
       "          0.81843384, -0.12847206]]),\n",
       " 'processes': [StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  SelectKBest(k='all', score_func=<function f_classif at 0x1149729b0>),\n",
       "  Pipeline(steps=[('selectkbest', SelectKBest(k=5, score_func=<function f_classif at 0x1149729b0>)), ('kneighborsclassifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n",
       "             weights='uniform'))]),\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  SelectKBest(k='all', score_func=<function f_classif at 0x1149729b0>),\n",
       "  Pipeline(steps=[('selectkbest', SelectKBest(k=5, score_func=<function f_classif at 0x1149729b0>)), ('kneighborsclassifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n",
       "             weights='uniform'))]),\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='ovr', n_jobs=1, penalty='l1', random_state=None,\n",
       "            solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  Pipeline(steps=[('selectkbest', SelectKBest(k=5, score_func=<function f_classif at 0x1149729b0>)), ('kneighborsclassifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n",
       "             weights='uniform'))])],\n",
       " 'test_score': 0.64800000000000002,\n",
       " 'train_score': 0.70199999999999996,\n",
       " 'y_test': 1618   -1\n",
       " 1905    1\n",
       " 1751    1\n",
       " 283     1\n",
       " 1313   -1\n",
       " 1803   -1\n",
       " 1262    1\n",
       " 613     1\n",
       " 1000   -1\n",
       " 724    -1\n",
       " 1601    1\n",
       " 1567    1\n",
       " 776    -1\n",
       " 847     1\n",
       " 1105    1\n",
       " 1595    1\n",
       " 1465    1\n",
       " 1983   -1\n",
       " 1448    1\n",
       " 1746    1\n",
       " 1643    1\n",
       " 797    -1\n",
       " 1020    1\n",
       " 1005   -1\n",
       " 1418   -1\n",
       " 1085   -1\n",
       " 1985    1\n",
       " 558    -1\n",
       " 1050   -1\n",
       " 846    -1\n",
       "        ..\n",
       " 1899    1\n",
       " 341     1\n",
       " 432    -1\n",
       " 1173    1\n",
       " 450     1\n",
       " 815     1\n",
       " 1185    1\n",
       " 1702    1\n",
       " 80     -1\n",
       " 1850   -1\n",
       " 1219    1\n",
       " 674    -1\n",
       " 214    -1\n",
       " 498    -1\n",
       " 1425   -1\n",
       " 908     1\n",
       " 679     1\n",
       " 1224    1\n",
       " 1374    1\n",
       " 61      1\n",
       " 1272    1\n",
       " 691    -1\n",
       " 1719    1\n",
       " 1468    1\n",
       " 892    -1\n",
       " 11     -1\n",
       " 67      1\n",
       " 579    -1\n",
       " 1245   -1\n",
       " 1688   -1\n",
       " Name: label, dtype: int64,\n",
       " 'y_train': 1891   -1\n",
       " 1591    1\n",
       " 408    -1\n",
       " 701     1\n",
       " 609    -1\n",
       " 713     1\n",
       " 1712   -1\n",
       " 1930   -1\n",
       " 1195   -1\n",
       " 1133    1\n",
       " 571    -1\n",
       " 1459   -1\n",
       " 1863    1\n",
       " 1685   -1\n",
       " 835     1\n",
       " 1481    1\n",
       " 1813   -1\n",
       " 154    -1\n",
       " 1215   -1\n",
       " 1064   -1\n",
       " 518    -1\n",
       " 1525    1\n",
       " 393     1\n",
       " 1010   -1\n",
       " 1175   -1\n",
       " 868    -1\n",
       " 714    -1\n",
       " 481    -1\n",
       " 551     1\n",
       " 1739   -1\n",
       "        ..\n",
       " 291     1\n",
       " 1003    1\n",
       " 1843    1\n",
       " 1695    1\n",
       " 1068    1\n",
       " 1090   -1\n",
       " 803     1\n",
       " 1555    1\n",
       " 925     1\n",
       " 1610   -1\n",
       " 1146   -1\n",
       " 1608    1\n",
       " 1632    1\n",
       " 1470    1\n",
       " 1305   -1\n",
       " 1637    1\n",
       " 1406    1\n",
       " 1837    1\n",
       " 353    -1\n",
       " 1882    1\n",
       " 660     1\n",
       " 509     1\n",
       " 1575   -1\n",
       " 200    -1\n",
       " 441     1\n",
       " 1197    1\n",
       " 537    -1\n",
       " 1798   -1\n",
       " 764    -1\n",
       " 885    -1\n",
       " Name: label, dtype: int64}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the anova classifier model\n",
    "df = general_model(anova_classifier, df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting up pipeline using KBest and KNeighbors\n",
    "this_pipeline = (SelectKBest(f_classif, k=5),\n",
    "                 KNeighborsClassifier(n_neighbors=17)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using KBest and KNN as estimators\n",
    "estimators = [('selectkbest', SelectKBest(f_classif, k=5)),\n",
    "              ('classifier', KNeighborsClassifier(n_neighbors=17))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('selectkbest', SelectKBest(k=5, score_func=<function f_classif at 0x1149729b0>)), ('classifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n",
      "           weights='uniform'))])\n",
      "Pipeline(steps=[('selectkbest', SelectKBest(k=5, score_func=<function f_classif at 0x1149729b0>)), ('kneighborsclassifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n",
      "           weights='uniform'))])\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(estimators)\n",
    "via_make = make_pipeline(*this_pipeline)\n",
    "print pipe\n",
    "print via_make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a List\n",
      "\n",
      "Indexed as a List\n",
      "('selectkbest', SelectKBest(k=5, score_func=<function f_classif at 0x1149729b0>))\n",
      "\n",
      "As a Dictionary\n",
      "SelectKBest(k=5, score_func=<function f_classif at 0x1149729b0>)\n"
     ]
    }
   ],
   "source": [
    "print('As a List')\n",
    "\n",
    "print('\\nIndexed as a List')\n",
    "print(pipe.steps[0])\n",
    "\n",
    "print('\\nAs a Dictionary')\n",
    "print(pipe.named_steps['selectkbest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('selectkbest', SelectKBest(k=5, score_func=<function f_classif at 0x1149729b0>)), ('classifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=17, p=2,\n",
       "           weights='uniform'))])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.set_params(classifier__n_jobs=-1)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__p': [1, 2],\n",
       " 'selectkbest__k': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting params\n",
    "params = dict(selectkbest__k=range(5,15),\n",
    "              classifier__p=[1,2])\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('selectkbest', SelectKBest(k=5, score_func=<function f_classif at 0x1149729b0>)), ('classifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=17, p=2,\n",
       "           weights='uniform'))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'selectkbest__k': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'classifier__p': [1, 2]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up Grid Search\n",
    "pipe_grid_search = GridSearchCV(pipe, param_grid=params, cv=5)\n",
    "pipe_grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "clf = general_model(GridSearchCV(LogisticRegression(penalty='l2'), param_grid))\n",
    "clf\n",
    "GridSearchCV(cv=None,\n",
    "       estimator=LogisticRegression(C=1.0, intercept_scaling=1, dual=False, fit_intercept=True,\n",
    "       penalty='l2', tol=0.0001),\n",
    "       fit_params={}, iid=True, n_jobs=1,\n",
    "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Conclusion:\n",
    "\n",
    "We constructed a Pipeline that uses SelectKBest to transform data\n",
    "We constructed a Pipeline that uses LogisticRegression to model data\n",
    "We constructed a Pipeline that uses KNearestNeighbors to model data\n",
    "Finally, we used Gridsearch optimal parameters for logistic regression and KNN\n",
    "\n",
    "We scored 70% Train and 64% Test which was another improvement"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
